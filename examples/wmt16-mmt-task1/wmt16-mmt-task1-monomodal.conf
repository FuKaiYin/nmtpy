[training]
model_type: attention
patience: 20
max_epochs: 100
valid_freq: 1000
valid_metric: meteor
decay_c: 1e-5
clip_c: 5
seed: 1235

[model]
tied_emb: 2way
layer_norm: True
shuffle_mode: trglen

emb_dropout: 0.2
ctx_dropout: 0.4
out_dropout: 0.4

rnn_dim: 100
embedding_dim: 100

weight_init: xavier
batch_size: 32
optimizer: adam
lrate: 0.0004

filter: bpe

n_words_src: 0
n_words_trg: 0

save_path: ~/nmtpy/models

[model.dicts]
src: ~/nmtpy/data/wmt16-task1/train.norm.max50.tok.lc.bpe.en.pkl
trg: ~/nmtpy/data/wmt16-task1/train.norm.max50.tok.lc.bpe.de.pkl

[model.data]
train_src     : ~/nmtpy/data/wmt16-task1/train.norm.max50.tok.lc.bpe.en
train_trg     : ~/nmtpy/data/wmt16-task1/train.norm.max50.tok.lc.bpe.de
valid_src     : ~/nmtpy/data/wmt16-task1/val.norm.tok.lc.bpe.en
# This is for loss computation in training. This is the actual reference file
valid_trg     : ~/nmtpy/data/wmt16-task1/val.norm.tok.lc.bpe.de
# This is the same file with BPE reverted to be used by nmt-translate for metric computation
valid_trg_orig: ~/nmtpy/data/wmt16-task1/val.norm.tok.lc.de
