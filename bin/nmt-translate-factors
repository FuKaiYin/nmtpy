#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""Translates a source file using a translation model."""
# Speed up beam search a little bit more with memory consumption tradeoff
import gc
gc.disable()

import os
import sys
import time
import json
import argparse
import importlib
import traceback
from multiprocessing import Process, Queue, cpu_count

from collections import OrderedDict, defaultdict

import numpy as np

from nmtpy.logger           import Logger
from nmtpy.metrics          import get_scorer
from nmtpy.nmtutils         import idx_to_sent
from nmtpy.sysutils         import *
from nmtpy.filters          import get_filter
from nmtpy.defaults         import INT, FLOAT

import nmtpy.cleanup as cleanup

# Setup the logger
Logger.setup(timestamp=False)
log = Logger.get()

# This is to avoid thread explosion. Allow
# each process to use a single thread.
os.environ["OPENBLAS_NUM_THREADS"] = "1"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["MKL_NUM_THREADS"] = "1"

# Force CPU
os.environ["THEANO_FLAGS"] = "device=cpu,optimizer_including=local_remove_all_assert"

def translate_model(rqueue, wqueue, pid, f_inits, f_nexts, beam_search, beam_size, nbest, suppress_unks, get_att_alphas=False, seed=1234, fact_constraints=[]):
    """Generates translations with beam search for single and ensemble models."""
    try:
        while True:
            # Get a sample
            req = rqueue.get()

            # Unpack sample idx and data_dict
            sample_idx, data_dict = req[0], req[1]

            # Get the translation, its score and alignments
            trans, score, align, trans_fact = beam_search(list(data_dict.values()), f_inits, f_nexts,
                        beam_size=beam_size,
                        get_att_alphas=get_att_alphas,
                        suppress_unks=suppress_unks,
                        fact_constraints=fact_constraints)

            # normalize scores according to sequence lengths
            score = score / np.array([len(s) for s in trans])

            # Sort the scores and take the best(s) idx(s)
            best_idxs = np.argsort(score)[:nbest]
            trans = np.array(trans)[best_idxs]
            trans_fact = np.array(trans_fact)[best_idxs]

            # Check for attention weights
            if align is not None:
                align = np.array(align)[best_idxs]

            # Send response back
            wqueue.put((sample_idx, trans, score[best_idxs], align, trans_fact))
    except Exception as e:
        traceback.print_exc()
        # Signal error back
        wqueue.put(None)

class Translator(object):
    """Starts worker processes and waits for the results."""
    def __init__(self, args):
        self.beam_size = args.beam_size

        # Always lists provided by argparse (nargs:'+')
        self.src_files = args.src_files
        self.ref_files = args.ref_files
        self.factors = args.factors
        self.constrained = args.constrained

        # Collect processed source sentences in here
        # for further exporting to json
        self.export = args.export

        # Fetch other arguments
        self.nbest          = args.nbest
        self.seed           = args.seed
        self.n_jobs         = args.n_jobs

        self.models         = []
        self.model_files    = args.models
        self.model_options  = []
        self.n_models       = len(self.model_files)

        self.suppress_unks  = args.suppress_unks

        # Post-processing filters
        self.filters        = []
        self.no_filters     = args.no_filters

        # Create worker process pool
        self.processes = [None] * self.n_jobs

    def set_model_options(self):
        for mfile in self.model_files:
            log.info('Initializing model %s' % os.path.basename(mfile))

            # Fetch options
            model_options = dict(np.load(mfile)['opts'].tolist())

            # Import the module
            self.__class = importlib.import_module("nmtpy.models.%s" % model_options['model_type']).Model

            # Create the model
            model = self.__class(seed=self.seed, logger=None, **model_options)
            
            # Load weights from the already loaded file
            model.load(mfile)

            model.set_dropout(False)

            model.build_sampler()

            self.models.append(model)
            self.model_options.append(model_options)

        # Sanity check for target vocabularies: they should all be same
        if self.n_models > 1:
            assert len(set([len(mopts['trg_dict']) for mopts in self.model_options])) == 1, \
                    "Target vocabularies should be the same for ensembling."
            assert len(set([len(mopts['trgfact_dict']) for mopts in self.model_options])) == 1, \
                    "Target factors vocabularies should be the same for ensembling."


        # Check for post-processing filter
        # NOTE: Filters should be the same for each model during ensembling
        if "filter" in self.model_options[0] and not self.no_filters:
            log.info("Hypotheses will be processed by the filters: '%s'" % model_options['filter'])
            filters = model_options['filter'].split(',')
            self.filters = [get_filter(f) for f in filters]

        # Get inverted dictionary from the model itself
        # NOTE: Target dictionary should be the same for each model during ensembling
        self.trg_idict = self.models[0].trg_idict
        self.trgfact_idict = self.models[0].trgfact_idict
            
        # Set contrained decoding
        # Load constraints on factor predictions (Franck).
        # The loaded file contains on each line elements
        # separated by space. The 1st element is the lemma,
        # all others are allowed factors for the lemma. Ex.:
        # dog noun+singular noun+plural
        self.fact_constraints = defaultdict(lambda: np.array(range(len(self.trgfact_idict))))
        if self.constrained:
            try:
                # dictionary with lemma and factors, each line lemma factor1 factor2 factor3
                const_file = open(self.constrained)
                print("Constrained search", const_file)
            except FileNotFoundError:
                print("File with factor constraints not found: unconstrained search")
                const_file = []
            for line in const_file:
                line = line.split()
                try:
                    lem = self.models[0].trg_dict[line[0]]
                except KeyError:
                    continue
                facts = [self.models[0].trgfact_dict.get(f, self.models[0].trgfact_dict['<unk>']) for f in line[1:]]
                self.fact_constraints[lem] = np.array(facts)

        #########################
        # Normal translation mode
        #########################
        if self.src_files is not None:
                # Pass the files to the model
                # NOTE: Not quite model agnostic way of doing things.
                self.models[0].data['valid_src'] = self.src_files[0]
                if 'valid_img' in self.models[0].data:
                    self.models[0].data['valid_img'] = self.src_files[1]
                if 'valid_srcfact' in self.models[0].data:
                    self.models[0].data['valid_srcfact'] = self.src_files[1]
                if self.factors and self.ref_files:
                    if len(self.ref_files) < 3:
                        log.info("ERROR: 3 references files are required (word-level, 1st_out-level and 2nd_out-level)")
                        exit()
                    self.models[0].data['valid_trg1'] = self.ref_files[1]
                    self.models[0].data['valid_trg2'] = self.ref_files[2]


        # Initialize model's validation data iterator
        self.models[0].load_valid_data(from_translate=True)

        # Set self.iterator to self.models[0].valid_iterator
        self.iterator = self.models[0].valid_iterator

        # All sentences
        self.n_sentences = self.iterator.n_samples

        # Assume validation data encoded in the model
        if self.src_files is None:
                log.info("No test data given, assuming validation dataset.")

                self.src_files = listify(self.models[0].data['valid_src'])

                # User may provide another reference set in 'valid_trg_orig' for example
                # with compound splitting reverted so that we can compute
                # the metrics correctly.
                # NOTE: May be avoided by using filters on reference sentences.
                if 'valid_srcfact' in self.models[0].data:
                    self.src_files.append(listify(self.models[0].data['valid_srcfact']))
                if "valid_trg_orig" in self.models[0].data:
                    self.ref_files = listify(self.models[0].data['valid_trg_orig'])
                else:
                    self.ref_files = listify(self.models[0].valid_ref_files)
                if self.factors:
                    self.ref_files = listify(self.models[0].data['valid_trg'])
                    self.ref_files.append(listify(self.models[0].data['valid_trg1']))
                    self.ref_files.append(listify(self.models[0].data['valid_trg2']))

        log.info('I will translate %d samples' % self.n_sentences)

        ###################
        # Print information
        ###################
        log.info("Source file(s)")
        for f in self.src_files:
            log.info("  %s" % f)

        if self.ref_files:
            log.info("Reference file(s)")
            for f in self.ref_files:
                log.info("  %s" % f)

    def start(self):
        # create input and output queues for processes
        write_queue = Queue()
        read_queue  = Queue()

        # Get theano functions and beam_search to pass to workers
        f_inits = [m.f_init for m in self.models]
        f_nexts = [m.f_next for m in self.models]
        beam_search = self.models[0].beam_search

        # Create processes
        for idx in range(self.n_jobs):
            self.processes[idx] = Process(target=translate_model,
                                          args=(write_queue, read_queue, idx, f_inits, f_nexts, beam_search, self.beam_size,
                                          self.nbest, self.suppress_unks, self.export,
                                          self.seed,
                                          self.fact_constraints))
            # Start process and register for cleanup
            self.processes[idx].start()
            cleanup.register_proc(self.processes[idx].pid)

        # Register the created processes to clean them after
        cleanup.register_handler(log)

        # Send data to worker processes
        for idx in range(self.n_sentences):
            write_queue.put((idx, next(self.iterator)))

        log.info("Distributed %d sentences to worker processes." % self.n_sentences)

        # Receive the results
        self.trans       = [None] * self.n_sentences
        if args.factors:
            self.trans_fact  = [None] * self.n_sentences
        self.scores      = [None] * self.n_sentences

        # Will be filled if --export is passed
        self.att_weights = [None] * self.n_sentences

        # Performance computation stuff
        start_time = per100_time = time.time()

        for i in range(self.n_sentences):
            # Get response from worker
            resp = read_queue.get()

            if resp is None:
                # Worker(s) failed
                log.info('One or more of the workers failed, exiting.')
                sys.exit(1)

            # This is the sample id of the processed sample
            sample_idx = resp[0]

            # Get the hypotheses, scores and attention weights if any
            hyps, self.scores[sample_idx], attw, trans_fact = resp[1:]

            # Did we receive attention weights from beam search?
            if attw is not None:
                self.att_weights[sample_idx] = attw[0]

            # FIXME: Improve
            # Place the hypotheses into their relevant places
            self.trans[sample_idx] = [idx_to_sent(self.trg_idict, hyp) for hyp in hyps]
            self.trans_fact[sample_idx] = [idx_to_sent(self.trgfact_idict, hyp) for hyp in trans_fact]

            # Print progress
            if (i+1) % 100 == 0:
                per100_time = time.time() - per100_time
                log.info("%4d/%d sentences completed (%.2f seconds)" % ((i+1), self.n_sentences, per100_time))
                per100_time = time.time()

        # Total time spent during beam search
        total_time      = time.time() - start_time
        sent_per_sec    = int(self.n_sentences / total_time)

        log.info("-------------------------------------------")
        log.info("Total decoding time: %3.3f seconds (%d sentences / sec)" % (total_time, sent_per_sec))

        # Compute word-based time statistics as well
        if self.nbest == 1:
            n_words         = float(sum([len(s[0].split(' ')) for s in self.trans]))
            word_per_sec    = int(n_words / total_time)
            log.info("~%d words / sec" % word_per_sec)

        # Stop workers
        for pidx in range(self.n_jobs):
            self.processes[pidx].terminate()

    # TODO give the translation as parameter
    def write_hyps(self, filename, n_out, dump_scores=False):
        if n_out == 1:
            trans = self.trans
        else:
            trans = self.trans_fact
        # Apply post-processing filters like compound stitching
        for i in range(len(trans)):
            # List of hyps (length 1 if nbest==1) per source sentence
            for j in range(len(trans[i])):
                inp = trans[i][j]
                for filt in self.filters:
                    inp = filt(inp)
                trans[i][j] = inp

        # Write file
        with open(filename, 'w') as f:
            if dump_scores:
                # We have a single hypothesis and a score for each sentence
                for idx, (tr, sc) in enumerate(zip(trans, self.scores)):
                    f.write("%d ||| %s ||| %.6f\n" % (idx, tr[0], sc))

            elif self.nbest > 1:
                # We have n hypotheses and n scores for each sentence
                for idx, (trs, scs) in enumerate(zip(trans, self.scores)):
                    for tr, sc in zip(trs, scs):
                        f.write("%d ||| %s ||| %.6f\n" % (idx, tr, sc))
            else:
                # Prepare and dump
                self.hyps = [s[0] for s in trans]
                hyps = "\n".join(self.hyps) + "\n"
                f.write(hyps)

    def compute_fact_score(self, hyp_files, scorers):
        """Computes evaluation metrics for the hypotheses."""
        results = {}
        for scorer in scorers:
            c = get_scorer(scorer)
            score = c.compute(self.ref_files[1], hyp_files[0])
            results['out1'] = (str(score), score.score)
            score_fact = c.compute(self.ref_files[2], hyp_files[1])
            results['out2'] = (str(score_fact), score_fact.score)
            if args.factors == 'evalf' or self.models[0].valid_ref_files == self.ref_files[1]:
                #results[scorer] = (str(score), 2*score.score+score_fact.score)
                results[scorer] = (str(score), score.score)
            else:
                # Calculate factors2word
                c = get_scorer('factors2word')
                score = c.compute(args.factors, hyp_files[0], hyp_files[1], self.ref_files[0])
                results[scorer] = (str(score), score.score)
        return results

    def dump_json(self, filename):
        """Export decoding data into json for further visualization."""
        metadata = OrderedDict()
        metadata['models']    = [os.path.basename(m) for m in self.model_files]
        metadata['beam_size'] = self.beam_size

        srcs    = []
        refs    = []
        samples = []

        # Reset iterator
        self.iterator.rewind()
        for i in range(self.n_sentences):
            data = next(self.iterator)
            if 'x' in data:
                srcs.append(idx_to_sent(self.models[0].src_idict, data['x'].flatten()))

        # Save metadata
        data = {'metadata' : metadata}

        # Open reference files
        all_refs = [open(f).read().strip().split("\n") for f in self.ref_files]
        n_refs = len(all_refs)

        mult_source = False
        if len(all_refs[0]) != len(srcs):
            # Multiple sources given
            mult_source = True

        # Collect reference sentences
        for sidx in range(self.n_sentences):
            sidx = sidx if not mult_source else sidx % len(all_refs[0])
            refs.append([all_refs[i][sidx] for i in range(n_refs)])

        # Add sources, targets, and references
        # if factors add trans_fact to the dump file
        # NOTE: att_weights are not cleared from BPE in terms of number of tokens
        if args.factors:
            for s, t, f, att in zip(srcs, self.trans, self.trans_fact, self.att_weights):
                sample = {'src' : s.split(' '), 'trg' : t[0].split(' '), 'trgfact' : f[0].split(' '), 'ref' : refs.pop(0), 'att': att}
                samples.append(sample)
        else:
            for s, t, att in zip(srcs, self.trans, self.att_weights):
                sample = {'src' : s.split(' '), 'trg' : t[0].split(' '), 'ref' : refs.pop(0), 'att': att}
                samples.append(sample)
        data['data'] = samples

        # Export the JSON
        def _default(obj):
            if isinstance(obj, np.ndarray):
                return obj.tolist()

        with open(filename, 'w') as f:
            json.dump(data, f, default=_default)

if __name__ == "__main__":
    # nmt-translate variant for Factored NMT model (attention_factors.py)
    parser = argparse.ArgumentParser(prog='nmt-translate-factors')
    parser.add_argument('-j', '--n-jobs'        , type=int, default=8,      help="Number of processes (default: 8, 0: Auto)")
    parser.add_argument('-b', '--beam-size'     , type=int, default=12,     help="Beam size (only for beam-search)")
    parser.add_argument('-N', '--nbest'         , type=int, default=1,      help="N for N-best output (only for beam-search)")
    parser.add_argument('-r', '--seed'          , type=int, default=1234,   help="Random number seed for sampling mode (default: 1234, not used)")

    parser.add_argument('-M', '--metrics'       , nargs='*',
                                                          default=['bleu'], help="bleu/meteor or path to external script.")
    parser.add_argument('-o', '--saveto'        , type=str, nargs='+',
                                                          default=None,     help="Output file(s) (if not given, only metrics will be printed)")
    parser.add_argument('-e', '--export'        , action='store_true',      help="Export all decoding process to json for visualization")
    parser.add_argument('-s', '--score'         , action='store_true',      help="Print scores of each sentence even nbest == 1")
    parser.add_argument('-u', '--suppress-unks' , action='store_true',      help="Don't produce <unk>'s in beam search")
    parser.add_argument('-d', '--no-filters'    , action='store_true',      help="Don't post-process translations using filters from config file.")

    parser.add_argument('-S', '--src-files'     , type=str, nargs='+',
                                                              default=None, help="Source data(s) in order: text,image (default: validation set)")
    parser.add_argument('-R', '--ref-files'     , type=str, nargs='+',
                                                              default=None, help="One or multiple reference files (default: validation set)")
    parser.add_argument('-m', '--models'        , nargs='+', required=True, help="Model files")
    # default factors is evalf meanwhile this nmt-translate-factors remains in a different file than nmt-translate
    parser.add_argument('-f', '--factors'       , type=str, default='evalf',help="Activate factors. The value can be evalf to evaluate just with the first output or the script to combine the 2 outputs (default=evalf)")
    parser.add_argument('-c', '--constrained'   , type=str, default=None,   help="Do constrained decoding providing a dictionary with lemma and factors, each line lemma factor1 factor2 factor3.")

    args = parser.parse_args()

    if args.n_jobs == 0:
        # Auto infer CPU count
        args.n_jobs = cpu_count()

    # Print some informations
    log.info("%d CPU processes - beam size = %2d" % (args.n_jobs, args.beam_size))
    log.info("Using %d model(s) for translation" % len(args.models))

    # Create translator object
    translator = Translator(args)
    translator.set_model_options()
    translator.start()

    out_file = args.saveto
    if not args.saveto:
        # Override this if given
        args.score = False
        hypf = get_temp_file(suffix=".nbest_hyps")
        out_file = [hypf.name]
        hypf2 = get_temp_file(suffix=".nbest_hyps2")
        out_file.append(hypf2.name)
        hypf.close()
        hypf2.close()

    # Export attentional informations if -o and -e are given
    if args.export and args.saveto:
        translator.dump_json("%s.json" % out_file[0])

    # Dump hypotheses
    # pass output file, number of output and scores
    translator.write_hyps(out_file[0], 1, args.score)
    translator.write_hyps(out_file[1], 2, args.score)

    # No need to compute metrics with nbest style files
    if args.nbest == 1 and translator.ref_files and not args.score:
        # Compute all metrics
        # Factors evaluation combining the 2 outputs
        results = translator.compute_fact_score(out_file, args.metrics)

        # NOTE: This dict is expected from nmt-translate for obtaining the validation results.
        print(results)

    # Report success
    sys.exit(0)
